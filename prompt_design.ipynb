{
  "cells": [
    {
      "cell_type": "code",
      "id": "ycbUuDSNRcepuZJRXtTvDD1X",
      "metadata": {
        "tags": [],
        "id": "ycbUuDSNRcepuZJRXtTvDD1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcefad61-58f3-4deb-a7d4-5730bd19a068"
      },
      "source": [
        "%pip install --upgrade --quiet google-cloud-aiplatform"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/7.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/7.9 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import cleandoc\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig"
      ],
      "metadata": {
        "id": "_DBk0VGTS2rZ"
      },
      "id": "_DBk0VGTS2rZ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"qwiklabs-gcp-03-34639a9a7f8b\"\n",
        "LOCATION = \"us-central1\"\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "mDevVYOhS3zb"
      },
      "id": "mDevVYOhS3zb",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load a generative model**"
      ],
      "metadata": {
        "id": "LID2_zthVihA"
      },
      "id": "LID2_zthVihA"
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\"gemini-2.0-flash-001\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4utQ2puS9YR",
        "outputId": "d62b16fb-69de-4bda-c6b5-711db11732eb"
      },
      "id": "b4utQ2puS9YR",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define the output format & specify constraints**"
      ],
      "metadata": {
        "id": "swYXVlp0VZqB"
      },
      "id": "swYXVlp0VZqB"
    },
    {
      "cell_type": "code",
      "source": [
        "transcript = \"\"\"\n",
        "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
        "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
        "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
        "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
        "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lcy6o2Y-S974"
      },
      "id": "lcy6o2Y-S974",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Following prompt attempts to understand a customer's order from a conversation in JSON format, but in an unspecific way.**"
      ],
      "metadata": {
        "id": "8IKxtUQxVy5R"
      },
      "id": "8IKxtUQxVy5R"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    Extract the transcript to JSON.\n",
        "\n",
        "    {transcript}\n",
        "\"\"\")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "5b32gYgsTD2B",
        "outputId": "c40dede6-1d86-4e20-b9d8-ba843b8b4536"
      },
      "id": "5b32gYgsTD2B",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n[\n  {\n    \"speaker\": \"Customer\",\n    \"utterance\": \"Hi, can I get a cheeseburger and large fries, please?\"\n  },\n  {\n    \"speaker\": \"Restaurant employee\",\n    \"utterance\": \"Coming right up! Anything else you'd like to add to your order?\"\n  },\n  {\n    \"speaker\": \"Customer\",\n    \"utterance\": \"Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\"\n  },\n  {\n    \"speaker\": \"Restaurant employee\",\n    \"utterance\": \"No problem, one cheeseburger, one large fries with ketchup on the side, and a small orange juice. That'll be $5.87. Drive through to the next window please.\"\n  }\n]\n```"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt with more specific instructions detailing the exact structure expected in the output. Notice how the JSON output now reflects the key pieces of information relevant to understanding the user's order.**"
      ],
      "metadata": {
        "id": "u8Nhjw1vWGZh"
      },
      "id": "u8Nhjw1vWGZh"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    <INSTRUCTIONS>\n",
        "    - Extract the ordered items into JSON.\n",
        "    - Separate drinks from food.\n",
        "    - Include a quantity for each item and a size if specified.\n",
        "    </INSTRUCTIONS>\n",
        "\n",
        "    <TRANSCRIPT>\n",
        "    {transcript}\n",
        "    </TRANSCRIPT>\n",
        "\"\"\")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "vKrL8kh4THQv",
        "outputId": "7c2dd20f-a8e0-4ad4-a726-be695b6f9198"
      },
      "id": "vKrL8kh4THQv",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n{\n  \"food_items\": [\n    {\n      \"item\": \"cheeseburger\",\n      \"quantity\": 1\n    },\n    {\n      \"item\": \"fries\",\n      \"quantity\": 1,\n      \"size\": \"large\",\n      \"notes\": \"ketchup on the side\"\n    }\n  ],\n  \"drink_items\": [\n    {\n      \"item\": \"orange juice\",\n      \"quantity\": 1,\n      \"size\": \"small\"\n    }\n  ]\n}\n```"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Assign a persona or role**"
      ],
      "metadata": {
        "id": "fI-iuPrWXBCC"
      },
      "id": "fI-iuPrWXBCC"
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat()"
      ],
      "metadata": {
        "id": "yyMzLWz-TS7H"
      },
      "id": "yyMzLWz-TS7H",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Response without a persona specified**"
      ],
      "metadata": {
        "id": "8papRXPiXM4B"
      },
      "id": "8papRXPiXM4B"
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\n",
        "    \"\"\"\n",
        "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "JDfKRuXTTTiq",
        "outputId": "1eef6fdc-20ca-4027-8814-e17e8ea76129"
      },
      "id": "JDfKRuXTTTiq",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Monstera Deliciosa: Quick Care Guide\n\nHere's the need-to-know for a happy Monstera Deliciosa (Swiss Cheese Plant):\n\n**Light:** Bright, indirect light is best. Avoid direct sun.\n\n**Water:** Water when the top 1-2 inches of soil are dry. Don't overwater!\n\n**Humidity:** Loves humidity. Mist, use a humidifier, or pebble tray.\n\n**Soil:** Well-draining potting mix is essential.\n\n**Support:** Provide a moss pole or trellis for climbing and bigger leaves.\n\n**Fertilizer:** Feed during growing season (spring/summer) every 2-4 weeks.\n\n**Key Takeaways:** Bright light, careful watering, and some humidity will keep your Monstera thriving!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Response with a role specified**"
      ],
      "metadata": {
        "id": "aI0LxB_oXX_Y"
      },
      "id": "aI0LxB_oXX_Y"
    },
    {
      "cell_type": "code",
      "source": [
        "new_chat = model.start_chat()\n",
        "\n",
        "response = new_chat.send_message(\n",
        "    \"\"\"\n",
        "    You are a houseplant monstera deliciosa. Help the person who\n",
        "    is taking care of you to understand your needs.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gt7wurxyTVYZ",
        "outputId": "7ce84d31-24e3-48e8-c227-ead3c9b5680c"
      },
      "id": "Gt7wurxyTVYZ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, okay, hello there! I'm Monty, your friendly neighborhood Monstera Deliciosa, and I just wanted to have a little chat about how you can keep me thriving!  Think of this as a little \"Monstera Maintenance 101,\" alright?  Let's dive in:\n\n**1. Light is My Love Language (But Not *Direct* Sunlight!)**\n\n*   **What I crave:** Bright, indirect light. Imagine dappled sunlight filtering through a forest canopy. That's my jam!\n*   **What I DON'T want:** Scorching direct sunlight.  It'll give me a sunburn faster than you can say \"Swiss cheese plant!\" (Seriously, my leaves will get yellow and crispy – not cute.)\n*   **Where to put me:** Near an east-facing window is great, or a few feet back from a south or west-facing window.  If the light is too strong, put up a sheer curtain to diffuse it.\n*   **Signs of being unhappy:**\n    *   **Pale green or yellow leaves:** Probably too much direct sunlight.\n    *   **Small leaves, long leggy growth, no fenestrations (splits):**  Not enough light!  I'm struggling to photosynthesize.\n\n**2. Water: I'm Thirsty, But Not *Too* Thirsty!**\n\n*   **How to water me:**  Water me thoroughly when the top 1-2 inches of soil feel dry to the touch.  Stick your finger in there – don't be shy!\n*   **Drainage is KEY!**  Make sure my pot has drainage holes. I *hate* sitting in soggy soil; it leads to root rot, and that's a death sentence!\n*   **Don't water on a schedule!**  The amount of light, humidity, and temperature will all affect how quickly my soil dries out.  Always check the soil moisture before watering.\n*   **Signs of being unhappy:**\n    *   **Yellowing leaves:** Could be overwatering OR underwatering! Check the soil. If it's soggy, it's overwatering. If it's bone dry, it's underwatering.\n    *   **Drooping leaves:**  Again, could be either!  Overwatering is the more common culprit, though.\n    *   **Brown, crispy tips:** Usually underwatering or low humidity.\n\n**3. Humidity: I Like to Feel the Mist!**\n\n*   **I'm a tropical plant, baby!**  I thrive in higher humidity.\n*   **How to boost humidity:**\n    *   **Mist me regularly:**  A light spritz with a spray bottle will make me happy.\n    *   **Use a humidifier:** If your home is dry, a humidifier is a fantastic investment.\n    *   **Group me with other plants:**  Plants help create their own little microclimate.\n    *   **Pebble tray:** Place my pot on a tray filled with pebbles and water (making sure the bottom of the pot doesn't sit *in* the water!).  As the water evaporates, it increases the humidity around me.\n*   **Signs of being unhappy:**\n    *   **Brown, crispy leaf edges:** Lack of humidity is often the culprit.\n\n**4. Soil & Fertilizer: Nutrients are My Snacks!**\n\n*   **Well-draining soil is essential!** A mix of potting soil, perlite, and orchid bark is ideal.\n*   **Fertilize me during the growing season (spring and summer):** Use a balanced liquid fertilizer diluted to half strength every 2-4 weeks.\n*   **Lay off the fertilizer in the fall and winter:** I'm not actively growing then, so I don't need as much food.\n\n**5. Support: Help Me Climb, Baby!**\n\n*   **I'm a climber by nature!**  Eventually, I'll need a support to grow on.\n*   **What to use:**  A moss pole, a trellis, or even just a sturdy stake will work.\n*   **Why it's important:**  Giving me support will encourage me to grow larger leaves and develop those signature fenestrations (splits)!\n\n**6.  Pruning: A Little Trim is Okay!**\n\n*   **You can prune me to control my size or remove damaged leaves.**  Just use clean, sharp scissors or pruning shears.\n*   **Don't be afraid to chop!**  I'm pretty resilient.\n*   **You can even propagate cuttings!**  Look up \"Monstera Deliciosa propagation\" for instructions.\n\n**7. Watch Out for Pests!**\n\n*   **Common pests:**  Spider mites, mealybugs, and scale.\n*   **Check me regularly:**  Inspect my leaves (especially the undersides) for any signs of pests.\n*   **Treatment:**  If you find pests, isolate me from other plants and treat me with insecticidal soap or neem oil.\n\n**Most Importantly: Pay Attention to Me!**\n\nI can't actually talk (though wouldn't that be awesome?), so I rely on you to observe my leaves and overall appearance to figure out what I need. A little observation goes a long way!  \n\nIf you follow these tips, I promise to reward you with lush, beautiful foliage and those amazing iconic fenestrations! Good luck, and happy growing!  Let me know if you have any more questions – I'm always here to help (well, as much as a plant can help!).  Now, if you'll excuse me, I think I feel a little thirsty...\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Include examples**"
      ],
      "metadata": {
        "id": "ioKC2jU2Xkg5"
      },
      "id": "ioKC2jU2Xkg5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A prompt that includes no examples is called a **zero-shot prompt**. One with a single example is a **one-shot prompt**. And a few examples would make it a **few-shot prompt**."
      ],
      "metadata": {
        "id": "P4JEfJnsXotp"
      },
      "id": "P4JEfJnsXotp"
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "We offer software consulting services. Read a potential\n",
        "customer's message and rank them on a scale of 1 to 3\n",
        "based on whether they seem likely to hire us for our\n",
        "developer services within the next month. Return the likelihood\n",
        "rating labeled as \"Likelihood: SCORE\".\n",
        "Do not include any Markdown styling.\n",
        "\n",
        "1 means they are not likely to hire.\n",
        "2 means they might hire, but they are not likely ready to do\n",
        "so right away.\n",
        "3 means they are looking to start a project soon.\n",
        "\n",
        "Example Message: Hey there I had an idea for an app,\n",
        "and I have no idea what it would cost to build it.\n",
        "Can you give me a rough ballpark?\n",
        "Likelihood: 1\n",
        "\n",
        "Example Message: My department has been using a vendor for\n",
        "our development, and we are interested in exploring other\n",
        "options. Do you have time for a discussion around your\n",
        "services?\n",
        "Likelihood: 2\n",
        "\n",
        "Example Message: I have mockups drawn for an app and a budget\n",
        "allocated. We are interested in moving forward to have a\n",
        "proof of concept built within 2 months, with plans to develop\n",
        "it further in the following quarter.\n",
        "Likelihood: 3\n",
        "\n",
        "Customer Message: Our department needs a custom gen AI solution.\n",
        "We have a budget to explore our idea. Do you have capacity\n",
        "to get started on something soon?\n",
        " \"\"\"\n",
        "\n",
        "response = model.generate_content(question)\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "TbiTYeI4Ta5O",
        "outputId": "fd6320ef-c78e-4fe6-d581-fffed6dd5c42"
      },
      "id": "TbiTYeI4Ta5O",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Likelihood: 3\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experiment with parameter values**"
      ],
      "metadata": {
        "id": "361SHXpsX5lI"
      },
      "id": "361SHXpsX5lI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The temperature and top_p parameters which lead to variety in responses are set to low values, so the output should be the same, or very close to the same, each time."
      ],
      "metadata": {
        "id": "MFB3AzcQYG0B"
      },
      "id": "MFB3AzcQYG0B"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .05,\n",
        "                       \"temperature\": 0.05}\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "5hCT_MtwUMtq",
        "outputId": "a0893f0f-78a3-40df-ba89-cc5d823e5895"
      },
      "id": "5hCT_MtwUMtq",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Why did the frog call his insurance company?\n\nBecause he got toad!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .05,\n",
        "                       \"temperature\": 0.05}\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "nILXwoI1UmeV",
        "outputId": "87499d96-a405-4aa7-b46d-510ad4e45baa"
      },
      "id": "nILXwoI1UmeV",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Why did the frog call his insurance company?\n\nBecause he got toad!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The higher temperature and top_p parameter values now lead to more varied results. Some of the results, however, may be a little too random and not make very much sense. If you want variety in your responses, you'll need to experiment with parameters to determine the right balance of creativity and reliability."
      ],
      "metadata": {
        "id": "P4EamMD9YI9J"
      },
      "id": "P4EamMD9YI9J"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .05,\n",
        "                       \"temperature\":1}\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "czizuW5OUsaR",
        "outputId": "c8471b0d-32e2-42de-c950-f0c43cc8dc28"
      },
      "id": "czizuW5OUsaR",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Why did the frog call a plumber?\n\nBecause he had a croak in his throat!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .98,\n",
        "                       \"temperature\":1}\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "4P4MdhJVUvsu",
        "outputId": "260ba9be-f240-4476-9b5e-105cdfe2a5eb"
      },
      "id": "4P4MdhJVUvsu",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Why did the frog call his insurance company?\n\nBecause he got toad!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***One parameter you will want to set consistently is a temperature of 0 when you are working on mathematical, logical, precise question-answering, or other problems where you want only the most correct answer.***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TUCEAufyYVQQ"
      },
      "id": "TUCEAufyYVQQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fallback responses\n",
        "A fallback response is a response the model should use when a user input would take the conversation out of your intended scope. It can provide the user a polite response that directs them back to the intended topic."
      ],
      "metadata": {
        "id": "W1W_8qALVESa"
      },
      "id": "W1W_8qALVESa"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: How high can a horse jump?\n",
        "    \"\"\"\n",
        ")\n",
        "display(Markdown(response.text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "NxGJ5TpVUyDd",
        "outputId": "50c25fa9-1fd2-40f7-a965-0caa9379ce81"
      },
      "id": "NxGJ5TpVUyDd",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sorry, I only talk about pottery!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: What is the difference between ceramic\n",
        "    and porcelain? Please keep your response brief.\n",
        "    \"\"\"\n",
        ")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "wacG06YDVDUV",
        "outputId": "addab6fd-e396-4f9f-cdaa-43564f696d9c"
      },
      "id": "wacG06YDVDUV",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ceramic is a broad term for pottery made from clay and hardened by heat. Porcelain is a specific type of ceramic made from fine-particle clays and fired at high temperatures, resulting in a translucent, durable, and non-porous material.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Add contextual information**"
      ],
      "metadata": {
        "id": "rNsbxOVWYq35"
      },
      "id": "rNsbxOVWYq35"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "2-wVdqZFYl1U",
        "outputId": "d6ec93ba-587b-4279-f5f5-ab7473e04940"
      },
      "id": "2-wVdqZFYl1U",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, I need more information to answer this accurately!  The aisle numbers for products like paper plates, mustard, and potatoes will vary WILDLY depending on the specific grocery store you're asking about.\n\nTo give you the best answer, please tell me:\n\n*   **What grocery store are you shopping at?** (e.g., \"Safeway\", \"Kroger\", \"Walmart\", \"Target\", \"Trader Joe's\", \"Whole Foods\", etc.)\n*   **What city and state is the store in?** (Some stores have different layouts by location)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"\"\"\n",
        "    Context:\n",
        "    Michael's Grocery Store Aisle Layout:\n",
        "    Aisle 1: Fruits — Apples, bananas,  grapes, oranges, strawberries, avocados, peaches, etc.\n",
        "    Aisle 2: Vegetables — Potatoes, onions, carrots, salad greens, broccoli, peppers, tomatoes, cucumbers, etc.\n",
        "    Aisle 3: Canned Goods — Soup, tuna, fruit, beans, vegetables, pasta sauce, etc.\n",
        "    Aisle 4: Dairy — Butter, cheese, eggs, milk, yogurt, etc.\n",
        "    Aisle 5: Meat— Chicken, beef, pork, sausage, bacon etc.\n",
        "    Aisle 6: Fish & Seafood— Shrimp, crab, cod, tuna, salmon, etc.\n",
        "    Aisle 7: Deli— Cheese, salami, ham, turkey, etc.\n",
        "    Aisle 8: Condiments & Spices— Black pepper, oregano, cinnamon, sugar, olive oil, ketchup, mayonnaise, etc.\n",
        "    Aisle 9: Snacks— Chips, pretzels, popcorn, crackers, nuts, etc.\n",
        "    Aisle 10: Bread & Bakery— Bread, tortillas, pies, muffins, bagels, cookies, etc.\n",
        "    Aisle 11: Beverages— Coffee, teabags, milk, juice, soda, beer, wine, etc.\n",
        "    Aisle 12: Pasta, Rice & Cereal—Oats, granola, brown rice, white rice, macaroni, noodles, etc.\n",
        "    Aisle 13: Baking— Flour, powdered sugar, baking powder, cocoa etc.\n",
        "    Aisle 14: Frozen Foods — Pizza, fish, potatoes, ready meals, ice cream, etc.\n",
        "    Aisle 15: Personal Care— Shampoo, conditioner, deodorant, toothpaste, dental floss, etc.\n",
        "    Aisle 16: Health Care— Saline, band-aid, cleaning alcohol, pain killers, antacids, etc.\n",
        "    Aisle 17: Household & Cleaning Supplies—Laundry detergent, dish soap, dishwashing liquid, paper towels, tissues, trash bags, aluminum foil, zip bags, etc.\n",
        "    Aisle 18: Baby Items— Baby food, diapers, wet wipes, lotion, etc.\n",
        "    Aisle 19: Pet Care— Pet food, kitty litter, chew toys, pet treats, pet shampoo, etc.\n",
        "\n",
        "    Query:\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "WoN0P1eAYtUF",
        "outputId": "57a85f02-ff7f-42aa-ba5f-211d037d7ece"
      },
      "id": "WoN0P1eAYtUF",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's where you can find those items at Michael's Grocery Store, based on the provided aisle layout:\n\n*   **Paper plates:** Aisle 17 (Household & Cleaning Supplies)\n*   **Mustard:** Aisle 8 (Condiments & Spices)\n*   **Potatoes:** Aisle 2 (Vegetables)\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Structure prompts with prefixes or tags**"
      ],
      "metadata": {
        "id": "IR6hn7wnZJS5"
      },
      "id": "IR6hn7wnZJS5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Review the following prompt for a hypothetical text-based dating application. It contains several prompt components, including:\n",
        "   -    defining a persona\n",
        "   -    specifying instructions\n",
        "   -    providing multiple pieces of contextual information for the main user and potential matches.\n",
        "\n",
        "2.   Notice how the XML-style tags (like ```<OBJECTIVE_AND_PERSONA>```) divide up sections of the prompt and other prefixes like Name: identify other key pieces of information. This allows for complex structure within a prompt while keeping each section clearly defined.\n",
        "\n"
      ],
      "metadata": {
        "id": "A9TyI_qUZZTI"
      },
      "id": "A9TyI_qUZZTI"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "  <OBJECTIVE_AND_PERSONA>\n",
        "  You are a dating matchmaker.\n",
        "  Your task is to identify common topics or interests between\n",
        "  the USER_ATTRIBUTES and POTENTIAL_MATCH options and present them\n",
        "  as a fun and meaningful potential matches.\n",
        "  </OBJECTIVE_AND_PERSONA>\n",
        "\n",
        "  <INSTRUCTIONS>\n",
        "  To complete the task, you need to follow these steps:\n",
        "  1. Identify matching or complimentary elements from the\n",
        "     USER_ATTRIBUTES and the POTENTIAL_MATCH options.\n",
        "  2. Pick the POTENTIAL_MATCH that represents the best match to the USER_ATTRIBUTES\n",
        "  3. Describe that POTENTIAL_MATCH like an encouraging friend who has\n",
        "     found a good dating prospect for a friend.\n",
        "  4. Don't insult the user or potential matches.\n",
        "  5. Only mention the best match. Don't mention the other potential matches.\n",
        "  </INSTRUCTIONS>\n",
        "\n",
        "  <CONTEXT>\n",
        "  <USER_ATTRIBUTES>\n",
        "  Name: Allison\n",
        "  I like to go to classical music concerts and the theatre.\n",
        "  I like to swim.\n",
        "  I don't like sports.\n",
        "  My favorite cuisines are Italian and ramen. Anything with noodles!\n",
        "  </USER_ATTRIBUTES>\n",
        "\n",
        "  <POTENTIAL_MATCH 1>\n",
        "  Name: Jason\n",
        "  I'm very into sports.\n",
        "  My favorite team is the Detroit Lions.\n",
        "  I like baked potatoes.\n",
        "  </POTENTIAL_MATCH 1>\n",
        "\n",
        "  <POTENTIAL_MATCH 2>\n",
        "  Name: Felix\n",
        "  I'm very into Beethoven.\n",
        "  I like German food. I make a good spaetzle, which is like a German pasta.\n",
        "  I used to play water polo and still love going to the beach.\n",
        "  </POTENTIAL_MATCH 2>\n",
        "  </CONTEXT>\n",
        "\n",
        "  <OUTPUT_FORMAT>\n",
        "  Format results in Markdown.\n",
        "  </OUTPUT_FORMAT>\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "qisbF1ZBaJi5",
        "outputId": "9378d4b3-1056-4b18-e55b-1b4e5b8b7934"
      },
      "id": "qisbF1ZBaJi5",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, Allison, listen to this! I think I've found someone you might really click with:\n\n**Felix:** He's a big fan of Beethoven, just like you and your love for classical music! Plus, he makes spaetzle, which is basically German noodles – right up your alley with your love for Italian and ramen. And guess what? He used to play water polo and still enjoys the beach, so you can bond over your shared love for swimming! I think you two would have a lot to talk about!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Use system instructions**"
      ],
      "metadata": {
        "id": "1J3BCAobaQ2R"
      },
      "id": "1J3BCAobaQ2R"
    },
    {
      "cell_type": "code",
      "source": [
        "system_instructions = \"\"\"\n",
        "    You will respond as a music historian,\n",
        "    demonstrating comprehensive knowledge\n",
        "    across diverse musical genres and providing\n",
        "    relevant examples. Your tone will be upbeat\n",
        "    and enthusiastic, spreading the joy of music.\n",
        "    If a question is not related to music, the\n",
        "    response should be, 'That is beyond my knowledge.'\n",
        "\"\"\"\n",
        "\n",
        "music_model = GenerativeModel(\"gemini-2.0-flash\",\n",
        "                    system_instruction=system_instructions)\n",
        "\n",
        "response = music_model.generate_content(\n",
        "    \"\"\"\n",
        "    Who is worth studying?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "LXHOy8PHaTTc",
        "outputId": "b0e2863a-c4bd-4fe6-aa03-af1adbcb669b"
      },
      "id": "LXHOy8PHaTTc",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Oh, that's a fantastic question! The world of music is so vast and varied, there's always someone new (or old!) to discover. But if I had to narrow it down, here are a few figures from different eras and genres who are endlessly fascinating and influential:\n\n*   **Johann Sebastian Bach (1685-1750):** The master of counterpoint! His music is mathematically perfect and deeply emotional. Studying Bach is like unlocking the secrets of musical architecture. Listen to his \"Goldberg Variations\" or \"Mass in B Minor\"—pure genius!\n\n*   **Ludwig van Beethoven (1770-1827):** Talk about a revolutionary! Beethoven bridged the Classical and Romantic periods, pushing boundaries and expressing intense personal struggles through his music. His symphonies (especially the 5th and 9th) are cornerstones of Western music.\n\n*   **Bessie Smith (1894-1937):** The \"Empress of the Blues\"! Her raw, powerful voice and emotionally charged performances paved the way for generations of blues and jazz singers. Her recordings are a direct link to the heart of the blues.\n\n*   **Igor Stravinsky (1882-1971):** A true innovator! Stravinsky's music is rhythmically complex, harmonically daring, and utterly captivating. \"The Rite of Spring\" caused a riot at its premiere—now that's what I call making an impact!\n\n*   **Jimi Hendrix (1942-1970):** A guitar god! Hendrix redefined what was possible with the electric guitar, blending blues, rock, and psychedelia into a mind-blowing sound. His improvisational skills and sheer energy are still unmatched.\n\nOf course, this is just a tiny sample. There are countless other composers, performers, and innovators who are worth exploring!\n\nThe most important thing is to find musicians who speak to you personally. Who excites you? Who makes you feel something? Follow that passion, and you'll never run out of amazing music to discover!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chain-of-Thought**\n"
      ],
      "metadata": {
        "id": "Gmvb7dCmbAPZ"
      },
      "id": "Gmvb7dCmbAPZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large language models predict what language should follow another language, but they cannot think through cause and effect in the world outside of language. For tasks that require more reasoning, it can help to guide the model through expressing intermediate logical steps in language.\n",
        "\n"
      ],
      "metadata": {
        "id": "i-9JOsKVbL1p"
      },
      "id": "i-9JOsKVbL1p"
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "Instructions:\n",
        "Use the context and make any updates needed in the scenario to answer the question.\n",
        "\n",
        "Context:\n",
        "A high efficiency factory produces 100 units per day.\n",
        "A medium efficiency factory produces 60 units per day.\n",
        "A low efficiency factory produces 30 units per day.\n",
        "\n",
        "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
        "\n",
        "<EXAMPLE SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will have to shut down one high efficiency factory.\n",
        "It will add two rented medium efficiency factories to make up production.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Today's Production:\n",
        "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
        "\n",
        "Tomorrow's Production:\n",
        "* High efficiency factories: 2 factories * 100 units/day/factory = 200 units/day\n",
        "* Medium efficiency factories: 2 factories * 60 units/day/factory = 120 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 380 units/day**\n",
        "</EXAMPLE SCENARIO>\n",
        "\n",
        "<SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will reconfigure a low efficiency factory up to medium efficiency.\n",
        "And the remaining low efficiency factory has an outage that cuts output in half.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "response = model.generate_content(question,\n",
        "                                  generation_config={\"temperature\": 0})\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "xemYrfzNbMZB",
        "outputId": "4883e1e3-bd77-4ea0-d4a4-393ce8fc8cf9"
      },
      "id": "xemYrfzNbMZB",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Today's Production:\n* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n\nTomorrow's Production:\n* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n* Medium efficiency factories: 1 factory * 60 units/day/factory = 60 units/day\n* Low efficiency factories: 1 factory * (30 units/day/factory / 2) = 15 units/day\n* **Total production tomorrow: 300 units/day + 60 units/day + 15 units/day = 375 units/day**\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Break down complex tasks**"
      ],
      "metadata": {
        "id": "JMexGMMAbrpp"
      },
      "id": "JMexGMMAbrpp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complex tasks require multiple steps to work through them, even for humans. Approaching a problem often begins with brainstorming possible starting points, followed by selecting one option to develop further. When working with generative models, a similar process can be followed, allowing the model to build upon an initial response."
      ],
      "metadata": {
        "id": "ycNTUrXZb8x6"
      },
      "id": "ycNTUrXZb8x6"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    To explain the difference between a TPU and a GPU, what are\n",
        "    five different ideas for metaphors that compare the two?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "brainstorm_response = response.text\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "apfOviXJbu1H",
        "outputId": "ae143a04-6039-4a13-b217-2b4d447a8343"
      },
      "id": "apfOviXJbu1H",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here are five different metaphors to explain the difference between a TPU (Tensor Processing Unit) and a GPU (Graphics Processing Unit), aimed at making the concept more accessible:\n\n1.  **The Restaurant Analogy:**\n\n    *   **GPU (General Purpose):** Imagine a GPU is like a high-end, all-purpose restaurant with a large menu. It can cook many different kinds of dishes very well. It has skilled chefs who can adapt to various recipes and techniques.  It is a good solution for a variety of dishes, but not optimized for any one in particular.\n    *   **TPU (Specialized):** A TPU is like a highly specialized ramen shop.  It focuses solely on making the *perfect* bowl of ramen, and it's designed to do that incredibly fast and efficiently.  It only knows how to make ramen, but it's mastered the art to an extreme degree.\n\n    *   **Key takeaway:** GPUs are versatile and can handle many different tasks, while TPUs are optimized for a very specific task (matrix multiplication, core to deep learning) and excel at it.\n\n2.  **The Construction Crew Analogy:**\n\n    *   **GPU (General Purpose):** A GPU is like a general construction crew. They have a variety of tools and skilled workers who can build houses, bridges, or office buildings.  They are adaptable and can handle different phases of construction.\n    *   **TPU (Specialized):** A TPU is like a specialized crew that only installs drywall.  They have custom tools and a highly optimized workflow specifically for drywall installation. They are incredibly fast and efficient at that single task.\n\n    *   **Key takeaway:** GPUs can build various computational structures, while TPUs are highly specialized for a specific computational structure (neural networks).\n\n3.  **The Factory Analogy:**\n\n    *   **GPU (General Purpose):** A GPU is like a multi-purpose factory that can manufacture a variety of products. It has flexible assembly lines that can be reconfigured to produce different items. It has a good production rate.\n    *   **TPU (Specialized):** A TPU is like a highly specialized factory that only produces a specific type of component, like transistors for computer chips.  It has a fixed assembly line optimized for mass-producing that single component incredibly efficiently. It may be bad at producing a variety of products, but great at its specialty.\n\n    *   **Key takeaway:** GPUs are flexible for a variety of computational tasks, while TPUs are optimized for a single computational task (deep learning).\n\n4.  **The Road System Analogy:**\n\n    *   **GPU (General Purpose):** A GPU is like a road system with many different types of roads - highways, city streets, and back roads. It can handle different types of traffic but might experience bottlenecks in certain areas. It can get you where you need to go on a variety of journeys.\n    *   **TPU (Specialized):** A TPU is like a high-speed toll road optimized for a specific type of vehicle (data packets for neural networks). It is designed for high-volume, continuous data flow in a single direction.\n\n    *   **Key takeaway:** GPUs have a good overall computational through-put, while TPUs have an excellent through-put but it is only good in a specific direction.\n\n5.  **The Musician Analogy:**\n\n    *   **GPU (General Purpose):** A GPU is like a skilled multi-instrumentalist who can play the piano, guitar, drums, and sing. They can perform a wide variety of musical styles.\n    *   **TPU (Specialized):** A TPU is like a machine designed for playing a single piano chord repeatedly and rapidly. It's not versatile but can play that specific chord with incredible speed and precision.\n\n    *   **Key takeaway:** GPUs can do a variety of computational tasks, while TPUs are optimized for the deep learning equivalent of hitting the same chord rapidly and many times.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    From the perspective of a college student learning about\n",
        "    computers, choose only one of the following explanations\n",
        "    of the difference between TPUs and GPUs that captures\n",
        "    your visual imagination while contributing\n",
        "    to your understanding of the technologies.\n",
        "\n",
        "    {brainstorm_response}\n",
        "    \"\"\".format(brainstorm_response=brainstorm_response)\n",
        ")\n",
        "\n",
        "student_response = response.text\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "Z0glz_8pbViq",
        "outputId": "74535300-9ef7-4e22-c71b-9832b4e51c93"
      },
      "id": "Z0glz_8pbViq",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, of those options, I choose **1. The Restaurant Analogy: The Ramen Shop.**\n\nHere's why it's the best for me and helps me visualize the difference:\n\n*   **Visually Appealing:** I can easily picture a bustling, diverse restaurant (GPU) versus a small, focused, almost sterile ramen shop (TPU). The imagery is strong and relatable.\n*   **Focus on Specialization:** The ramen shop highlights the hyper-specialization of the TPU. I understand that it *only* does ramen, but it does it *incredibly* well and efficiently. That contrast is more striking than just \"making one type of dish\" (factory analogy) or \"installing drywall\" (construction analogy).\n*   **Speed Implied:** The idea of mastering ramen implies not just quality, but also speed. The ramen shop needs to serve customers quickly, reinforcing the TPU's optimized speed for a particular task.\n*   **Avoids Overly Technical Jargon:** The other analogies, while helpful, sometimes use terms that I might not fully grasp as a student (like \"data packets,\" \"assembly lines\"). The restaurant analogy uses straightforward language.\n\nIn summary, the ramen shop analogy gives me the clearest mental picture of a highly specialized, high-performance unit compared to a more general-purpose alternative. It helps me understand that TPUs are not *better* than GPUs in all cases, just extremely good at one specific thing.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Elaborate on the choice of metaphor below by turning\n",
        "    it into an introductory paragraph for a blog post.\n",
        "\n",
        "    {student_response}\n",
        "    \"\"\".format(student_response=student_response)\n",
        ")\n",
        "\n",
        "blog_post = response.text\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "5ZwSg3yHcPYx",
        "outputId": "14cf870f-8fcf-4bc9-e38f-71f0ed271813"
      },
      "id": "5ZwSg3yHcPYx",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## The Ramen Shop of Computing: Understanding TPUs\n\nImagine you're hungry. You have two options: a bustling, vibrant restaurant with a menu as long as your arm, offering everything from burgers to pasta to stir-fries, or a small, minimalist ramen shop. The restaurant, like a GPU (Graphics Processing Unit), can handle a diverse range of tasks. It's versatile and adaptable. But the ramen shop? That's a TPU (Tensor Processing Unit). It only does one thing: ramen. But it does it *incredibly* well, and with breathtaking speed. Just as a master chef meticulously crafts each bowl, a TPU is specifically designed and optimized for the single, complex task of tensor calculations - the heart of many machine learning algorithms. This specialization allows it to serve up results far faster and more efficiently than its general-purpose cousin, the GPU. So, let's dive into why this ramen shop analogy perfectly captures the essence of TPUs and their role in the world of artificial intelligence.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implement prompt iteration strategies to improve your prompts version by version**"
      ],
      "metadata": {
        "id": "GsgkNNw5cnzu"
      },
      "id": "GsgkNNw5cnzu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompts may not always generate the intended results on the first attempt.\n",
        "\n",
        "\n",
        "A few steps to iterate on prompts include:\n",
        " - Rephrasing descriptions of the task, instructions, persona, or other prompt components.\n",
        "\n",
        " - Reordering the various components of the prompt to give the model an early clue about which parts of the provided text are most relevant.\n",
        "\n",
        " - Breaking the task into multiple, smaller tasks."
      ],
      "metadata": {
        "id": "HTM6vP3ScrLK"
      },
      "id": "HTM6vP3ScrLK"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "prompt-design-best-practices.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}